{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabadawy/DeepLearning/blob/master/Labs/Lab4/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Text Generation with RNNs\n",
        "\n",
        "In this assignment, you will explore text generation using deep learning models, specifically Recurrent Neural Networks (RNNs). Text generation is a fascinating task where the goal is to train a model that can predict the next word in a sequence, ultimately generating coherent and contextually accurate sentences or paragraphs.\n",
        "\n",
        "You will work with a corpus of text data provided in the tutorial (from TensorFlow), which contains a large collection of Shakespeare’s works. Your objective is to implement a text generation model using RNNs and to evaluate its performance based on various metrics.\n",
        "\n",
        "<img src=\"https://drek4537l1klr.cloudfront.net/teofili2/Figures/03fig09_alt.jpg\" alt=\"Drawing\"/>\n",
        "\n",
        "**The models include:**\n",
        "- Deep RNN (LSTM)\n",
        "- Deep RNN (GRU)\n",
        "- Bidirectional RNN\n",
        "\n",
        "\n",
        "**Evaluation:**\n",
        "\n",
        "Evaluate the performance of the text generation model using several metrics:\n",
        "- Perplexity: A measure of how well the probability distribution predicted by the model aligns with the actual distribution of words in the text.\n",
        "- Generated Text Quality: Subjectively evaluate the quality of the generated text by considering grammar, coherence, and creativity. This can be done by visually inspecting the generated sequences.\n",
        "- BLUE Score: Character-level BLEU: BLEU can be applied at the character level by treating each character as an n-gram. This is particularly useful when the task involves generating character sequences (like poetry, code, or fine-grained character-based generation).\n",
        "For example, the sequence “hello” could be evaluated by comparing 1-grams (e.g., “h”, “e”, “l”, “l”, “o”) or higher-order n-grams (e.g., “he”, “el”, “ll”, “lo”).\n",
        "\n",
        "**Goals:**\n",
        "\n",
        "- Compare the performance of GRU and LSTM. Is there a significant difference in the results?\n",
        "- Compare the performance of the unidirectional and bidirectional RNN models.\n",
        "Which model produces better results?\n",
        "- Discuss the impact of bidirectional RNNs on text generation tasks."
      ],
      "metadata": {
        "id": "UR9ndGEKbi7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import TensorFlow and other libraries"
      ],
      "metadata": {
        "id": "zWHyOoc1kRzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "RvnvzfT8kU2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "RVCWJVSUkBi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shakespeare dataset used for text generation contains a collection of works by William Shakespeare, primarily in the form of plays and sonnets. The text is used to train language models, offering an ideal example for character-level modeling due to its rich, complex language. The dataset is often preprocessed by tokenizing it into individual characters, enabling models to learn the sequential relationships between characters for generating coherent and contextually appropriate text.\n",
        "\n",
        "To access the dataset, you can use TensorFlow's get_file method as follow:\n",
        "\n",
        "You can inspect the dataset on [Kaggle](https://www.kaggle.com/datasets/adarshpathak/shakespeare-text/data)."
      ],
      "metadata": {
        "id": "w-EJ5MXEkFOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQoXSAoflWB9",
        "outputId": "59dedab2-aa76-443f-cbaa-1c2c823a54df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbs11e97l08V",
        "outputId": "18482125-d545-4e78-87dc-5daed1959ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the text"
      ],
      "metadata": {
        "id": "KaGt1DN6mhB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ],
      "metadata": {
        "id": "WmZYdXLqwgCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "id": "TudtcfRXmlJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ec7c37-4f0a-4775-a199-438d25ea40d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now create the [`tf.keras.layers.StringLookup`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) layer:\n",
        "\n",
        "Since the goal of this assignment is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use tf.keras.layers.StringLookup(..., invert=True).\n",
        "\n",
        "Note: Here instead of passing the original vocabulary generated with sorted(set(text)) use the get_vocabulary() method of the tf.keras.layers.StringLookup layer so that the [UNK] tokens is set the same way."
      ],
      "metadata": {
        "id": "3ekRBgD3xX44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_string_lookup_layers(vocab):\n",
        "    \"\"\"\n",
        "    Creates StringLookup layers for encoding characters to IDs and decoding IDs back to characters.\n",
        "\n",
        "    Args:\n",
        "        vocab (list): List of unique characters in the dataset.\n",
        "\n",
        "    Returns:\n",
        "        ids_from_chars (tf.keras.layers.StringLookup): Converts characters to IDs.\n",
        "        chars_from_ids (tf.keras.layers.StringLookup): Converts IDs back to characters.\n",
        "    \"\"\"\n",
        "    ids_from_chars = '''TO DO'''\n",
        "    chars_from_ids = '''TO DO'''\n",
        "    return ids_from_chars, chars_from_ids"
      ],
      "metadata": {
        "id": "S4nkBgY80jKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids, chars_from_ids):\n",
        "    \"\"\"\n",
        "    Converts a sequence of character IDs into a human-readable string.\n",
        "\n",
        "    Args:\n",
        "        ids (tf.Tensor): Tensor of character IDs.\n",
        "        chars_from_ids (tf.keras.layers.StringLookup): StringLookup layer to decode IDs.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Decoded string.\n",
        "    \"\"\"\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "yy3htlAl2LpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create training examples and targets"
      ],
      "metadata": {
        "id": "WfPugm5F2tit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "LgO2xqIl26Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "-zlTqVW-4JYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "M5nUqJy94jz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Text to Numerical Sequences"
      ],
      "metadata": {
        "id": "cYBKXEO2-jo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize mapping layers\n",
        "vocab = sorted(set(text))\n",
        "ids_from_chars, chars_from_ids = get_string_lookup_layers(vocab)\n",
        "\n",
        "\n",
        "\n",
        "# Convert text to character IDs\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "\n",
        "# Create a dataset from the character IDs\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "m07YEPAn95W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Sequences for Training"
      ],
      "metadata": {
        "id": "m4SjKO_U-3_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # Length of each training sequence\n",
        "\n",
        "# Batch sequences (each sequence is seq_length + 1)\n",
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "# Map dataset to input-target format\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Print sample input-output pairs\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", tf.strings.reduce_join(chars_from_ids(input_example)).numpy().decode('utf-8'))\n",
        "    print(\"Target:\", tf.strings.reduce_join(chars_from_ids(target_example)).numpy().decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfiiwVFB-5lc",
        "outputId": "b203961b-47e0-426a-88a9-51b6b2909399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "Target: irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create RNN Models"
      ],
      "metadata": {
        "id": "dI6ui5GUmmTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training our text generation models, we need to set up key parameters and prepare the dataset. These parameters control the size of batches, buffer size for shuffling, embedding dimensions, and the number of units in the recurrent layer."
      ],
      "metadata": {
        "id": "xURojLcPEE7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customizing Parameters for Optimization\n",
        "\n",
        "- Increase RNN_UNITS if the model is underfitting (not capturing enough detail).\n",
        "- Decrease BATCH_SIZE if memory usage is too high (e.g., when using large RNN units).\n",
        "- Adjust EMBEDDING_DIM to experiment with the quality of learned character representations.\n",
        "\n",
        "Tip: Start with these values, then fine-tune based on model performance and available computational resources!"
      ],
      "metadata": {
        "id": "qGISBf1UEYFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "# Prepare dataset for training\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE)\n",
        "           .batch(BATCH_SIZE, drop_remainder=True)\n",
        "           .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "M4WOh946DkY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-Based Model"
      ],
      "metadata": {
        "id": "dVgheHjADca6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) designed to capture long-range dependencies in sequential data. Unlike traditional RNNs, which struggle with vanishing gradients, LSTMs use gates (input, forget, and output gates) to regulate the flow of information, making them highly effective for text generation tasks.\n",
        "\n",
        "This function defines an LSTM-based neural network for text generation.\n",
        "\n",
        "**Function Overview:**\n",
        "* [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): This is the input layer, consisting of a trainable lookup table that maps the numbers of each character to a vector with `embedding_dim` dimensions.\n",
        "* [`tf.keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Our LSTM network, with size `units=rnn_units`.\n",
        "* [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): The output layer, with `vocab_size` outputs.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab1/img/lstm_unrolled-01-01.png\" alt=\"Drawing\"/>"
      ],
      "metadata": {
        "id": "KkpCV_AOFUv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  \"\"\"TO DO\"\"\"\n",
        "  \"\"\"\n",
        "    Builds an LSTM-based text generation model.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size (int): Number of unique characters in the vocabulary.\n",
        "    - embedding_dim (int): Dimension of the word/character embeddings.\n",
        "    - rnn_units (int): Number of units in the LSTM layer.\n",
        "    - batch_size (int): Number of sequences processed in parallel.\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.Model: Compiled LSTM model.\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "Mfg_nDPJ4Iph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customization & Optimization**\n",
        "\n",
        "These experiments are required to improve model performance. You must conduct the following experiments and document your findings:\n",
        "\n",
        "- Increase RNN_UNITS : This enhances the model’s ability to recognize deeper patterns but may increase training time.\n",
        "- Experiment with EMBEDDING_DIM : Adjusting this can improve the quality of character representation.\n",
        "- Stack multiple LSTM layers : This can help the model understand text more effectively.\n",
        "\n",
        "**Submission Requirements**\n",
        "\n",
        "- Perform at least three trials varying the parameters above.\n",
        "- Keep the most performing plots and outputs in your Jupyter Notebook.\n"
      ],
      "metadata": {
        "id": "lPG4uS2rGVZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU-Based Model"
      ],
      "metadata": {
        "id": "7zitjaXenJJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is GRU?\n",
        "\n",
        "Gated Recurrent Units (GRU) are a simplified version of LSTMs that combine the forget and input gates into a single update gate. This makes GRUs:\n",
        "\n",
        "- Faster and more efficient than LSTMs\n",
        "- Perform well on shorter sequences\n",
        "- Require fewer computational resources\n",
        "\n",
        "**Model Comparison (LSTM vs. GRU)**\n",
        "\n",
        "To evaluate the differences between LSTM and GRU models, compare them using:\n",
        "\n",
        "- Model Size & Number of Parameters : Use model.summary() to check the number of trainable parameters in each model.\n",
        "- Training Speed & Efficiency : Measure training time per epoch for both models."
      ],
      "metadata": {
        "id": "yizzA4azG6rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gru_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    \"\"\"\n",
        "    Builds a GRU-based text generation model.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size (int): Number of unique characters in the vocabulary.\n",
        "    - embedding_dim (int): Dimension of the word/character embeddings.\n",
        "    - rnn_units (int): Number of units in the GRU layer.\n",
        "    - batch_size (int): Number of sequences processed in parallel.\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.Model: Compiled GRU model.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "YYGsUUt9IHiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional Model\n",
        "\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/342646275/figure/fig4/AS:962238546464790@1606426955772/Comparison-between-LSTM-and-Bi-LSTM-networks-recreated-after-33.png\" alt=\"Drawing\"/>\n"
      ],
      "metadata": {
        "id": "V5WHnOJzmxNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Bidirectional RNN?**\n",
        "\n",
        "A Bidirectional Recurrent Neural Network (BiRNN) is an extension of a standard RNN that processes input sequences in both forward and backward directions. This means that at each time step, the model considers both past (left-to-right) and future (right-to-left) context, leading to better performance in many sequence-related tasks, including text generation.\n",
        "\n",
        "**Why Use Bidirectional RNNs?**\n",
        "\n",
        "Bidirectional RNNs are used because they capture dependencies in the input data from both directions, which is crucial for understanding context. This makes them particularly useful in tasks like language processing, where the meaning of a word can depend on both the words that come before and after it.\n",
        "\n",
        "For example, in the sentence \"The cat sat on the mat,\" the word \"mat\" is influenced by both the preceding words (\"The cat sat on the\") and what could potentially follow (e.g., \"and looked at the mouse\").\n",
        "\n",
        "Example\n",
        "\n",
        "Let's consider a sentence: \"He opened the door.\"\n",
        "\n",
        "Simple RNN: It processes the sentence from left to right, word by word:\n",
        "\n",
        "He → opened → the → door\n",
        "Each word is processed based on the previous word.\n",
        "\n",
        "Bidirectional RNN: It processes the sentence in both directions:\n",
        "\n",
        "Forward pass: He → opened → the → door\n",
        "\n",
        "Backward pass: door → the → opened → He\n",
        "\n",
        "The output at each time step is a combination of the information from both the forward and backward passes, allowing the network to understand the context better. For instance, knowing that \"door\" follows \"the\" helps confirm that \"opened\" likely refers to a physical action, not a metaphorical one."
      ],
      "metadata": {
        "id": "3JFUjj1II4Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Choosing the Base RNN for Bidirectional Processing:**\n",
        "\n",
        "One key flexibility of Bidirectional RNNs is that you can choose any recurrent architecture (LSTM, GRU, Simple RNN) as the base model.\n",
        "\n",
        "- Bidirectional LSTM: Best for handling long-range dependencies.\n",
        "- Bidirectional GRU: More computationally efficient than LSTM.\n",
        "- Bidirectional Simple RNN: Less commonly used due to vanishing gradient issues.\n",
        "\n",
        "Note: To determine the best base model for the Bidirectional RNN, you must evaluate the results from your previous experiments with different architectures (eg. LSTM and GRU).\n",
        "\n",
        "Review previous results from training LSTM and GRU models.\n",
        "Compare their performance in terms of:\n",
        "\n",
        "- Training time per epoch\n",
        "- Model size (number of parameters)\n",
        "- Loss and accuracy on validation data\n",
        "- Quality of generated text\n",
        "\n",
        "Based on your findings, select the best-performing model as the base architecture for the Bidirectional RNN.\n"
      ],
      "metadata": {
        "id": "Cie7twv5J__0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_birnn_model(vocab_size, embedding_dim, rnn_units, batch_size, rnn_type=\"LSTM\"):\n",
        "    \"\"\"TO DO\"\"\"\n",
        "    \"\"\"\n",
        "    Builds a Bidirectional RNN-based text generation model.\n",
        "\n",
        "    Parameters:\n",
        "    - vocab_size (int): Number of unique characters in the vocabulary.\n",
        "    - embedding_dim (int): Dimension of the word/character embeddings.\n",
        "    - rnn_units (int): Number of units in the RNN layer.\n",
        "    - batch_size (int): Number of sequences processed in parallel.\n",
        "    - rnn_type (str): Type of RNN to use as the base (options: \"LSTM\", \"GRU\").\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.Model: Compiled Bidirectional RNN model.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "uyeUX3wqKutr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Models"
      ],
      "metadata": {
        "id": "tDsQJY7wnZxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Selection"
      ],
      "metadata": {
        "id": "N_dYS7PZHPzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose model type\n",
        "model_type = \"LSTM\"  # Change to \"GRU\" or \"BiLSTM\"\n",
        "\n",
        "if model_type == \"LSTM\":\n",
        "    model = build_lstm_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "elif model_type == \"GRU\":\n",
        "    model = build_gru_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "elif model_type == \"BiLSTM\":\n",
        "    model = build_birnn_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "\n",
        "print(f\"Using {model_type} model for training.\")"
      ],
      "metadata": {
        "id": "Mmcr-HmQnepP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile & Train the Model"
      ],
      "metadata": {
        "id": "h5UaTlKZLQxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Define checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "# Train the model\n",
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "0mEIvU_3LPyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text Using the Model"
      ],
      "metadata": {
        "id": "Ahq6S6erLqBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStepTextGenerator(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Initializes the OneStepTextGenerator.\n",
        "\n",
        "        Parameters:\n",
        "        - model: Trained text generation model (e.g., LSTM or GRU).\n",
        "        - chars_from_ids: Function mapping character IDs to characters.\n",
        "        - ids_from_chars: Function mapping characters to their respective IDs.\n",
        "        - temperature (float): Controls randomness in text generation.\n",
        "          -> Higher temperature (> 1.0): Produces more diverse and unpredictable text.\n",
        "          -> Lower temperature (< 1.0): Makes the model more confident but results in repetitive text.\n",
        "          -> Temperature = 1.0: Standard behavior without biasing randomness.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def generate_text(self, start_string, num_generate=1000):\n",
        "        \"\"\"\n",
        "        Generates text one character at a time based on a starting string.\n",
        "\n",
        "        Parameters:\n",
        "        - start_string (str): Initial text prompt.\n",
        "        - num_generate (int): Number of characters to generate.\n",
        "\n",
        "        Returns:\n",
        "        - str: Generated text.\n",
        "        \"\"\"\n",
        "        # Convert start string to tensor representation\n",
        "        input_eval = tf.expand_dims(self.ids_from_chars(tf.strings.unicode_split(start_string, 'UTF-8')), 0)\n",
        "        text_generated = []\n",
        "\n",
        "        # Initial hidden state (None allows automatic initialization)\n",
        "        states = None\n",
        "\n",
        "        for _ in range(num_generate):\n",
        "            # Get model predictions and hidden state\n",
        "            predictions, states = self.model(input_eval, states=states, return_state=True)\n",
        "\n",
        "            # Adjust predictions using temperature\n",
        "            predictions = predictions[:, -1, :] / self.temperature\n",
        "\n",
        "            # Sample the next character ID from a probability distribution\n",
        "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "            # Use the predicted ID as next input\n",
        "            input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "            # Convert ID back to character and append to output\n",
        "            text_generated.append(self.chars_from_ids(predicted_id).numpy().decode('utf-8'))\n",
        "\n",
        "        return start_string + ''.join(text_generated)\n",
        "\n",
        "# Create a text generator instance\n",
        "text_generator = OneStepTextGenerator(model, chars_from_ids, ids_from_chars, temperature=0.8)\n",
        "\n",
        "# Generate sample text\n",
        "generated_text = text_generator.generate_text(\"ROMEO: \", num_generate=500)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "MssY2FXJLs1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model Performance"
      ],
      "metadata": {
        "id": "fhvfs2NkL7kw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Perplexity (PP)**\n",
        "\n",
        "TODO: Measures uncertainty in predicting next character for each model and leave the results in the Notebook.\n",
        "\n",
        "***Lower values = better model***"
      ],
      "metadata": {
        "id": "ucKVOi9KMHuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(logits, labels):\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "    return np.exp(np.mean(loss))"
      ],
      "metadata": {
        "id": "7i38HrLPMAM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Text Coherence & Fluency**\n",
        "\n",
        "Subjective evaluation by examining generated text\n",
        "\n",
        "**TODO: Generate at least 3 Samples (one from each model) and leave them in the Notebook.**"
      ],
      "metadata": {
        "id": "xCd0Am-KMaCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3- BLEU Score**\n",
        "\n",
        "TODO: Measures similarity between generated and real text for each model and leave the results in the Notebook."
      ],
      "metadata": {
        "id": "pPFZTX_INVsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(reference, generated):\n",
        "    return sentence_bleu([list(reference)], list(generated))"
      ],
      "metadata": {
        "id": "FSUisLXDM3Is"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}