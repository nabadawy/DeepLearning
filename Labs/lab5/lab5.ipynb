{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_62mxdDlvTa"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/sherifmost/DeepLearning/blob/master/Labs/lab5/lab5.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwy6r8HHrgAs"
      },
      "source": [
        "# Assignment 4: Visual Image Caption Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMy3F9NUpdgi"
      },
      "source": [
        "![CNN RNN Image Captioning](https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab5/Cover.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylFcVtlKrrwx"
      },
      "source": [
        "## 5.1 Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-vYRV6Mr4Sk"
      },
      "source": [
        "In this Assignment you will build an encoder-decoder architecture with attention to address the task of image captioning on the Flickr 8k dataset.\n",
        "\n",
        "The model comprises the following main components:\n",
        "\n",
        "\n",
        "1.   Encoder: we will be using an InceptionV3 model pre-trained on imagenet for extracting the image features.\n",
        "2.   Attention Model: we will implement the [Bahdanau Attention Mechanism](https://machinelearningmastery.com/the-bahdanau-attention-mechanism/) and get introduced to how the mathematical operations learned in class are implemented using Tensorflow operations.\n",
        "3.   Decoder: we will be using an RNN-based decoder using LSTM.\n",
        "\n",
        "\n",
        "The dataset consists of 6000 training examples and 1000 testing examples, with different image sizes. **In this assignment, you will also learn how to deal with loading such a large dataset to the RAM without crashing it.**\n",
        "\n",
        "We will rely on qualitative evaluation of the model by making sure that the captions make sense on a random test image at the end!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C9ccyyCtq8n"
      },
      "source": [
        "**IMPORTANT NOTE:** You have to change runtime type on Google Colab to GPU since this assignment requires much computation resources and it will run very slowly on CPU (Default runtime type)\n",
        "\n",
        "Click on \"Runtime\" => \"Change runtime type\" => make sure that GPU is selected in the \"Hardware accelerator\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXQOsmZj_OET"
      },
      "source": [
        "Now lets walk through the code, and tell you the parts you need to fill.\n",
        "\n",
        "**MAKE SURE YOU KEEP ALL THE OUTPUTS FOR THE SUBMISSION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbuTZrb6sEPE"
      },
      "source": [
        "## 5.2 Problem Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0xhJ9LCsNjL"
      },
      "source": [
        "### 5.2.1 Importing Needed Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2KnVA0E-Two"
      },
      "outputs": [],
      "source": [
        "# Import needed packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "# This library is used for providing regex functionalities\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxp8lLUi_idH"
      },
      "source": [
        "### 5.2.2 The Flickr8k Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5rxN2GlEiyv"
      },
      "source": [
        "In this assignment, we will use the [Flickr8k dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k) which consists of 8000 images with 5 captions for each image.\n",
        "\n",
        "Let's download the dataset to the session and inspect it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-M6Czq186Vo"
      },
      "source": [
        "#### 5.2.2.1 Downloading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_j4e9xC_yWN"
      },
      "source": [
        "The dataset is obtained from this [github repo](https://github.com/jbrownlee/Datasets/releases/) containing the original dataset, and is downloaded to the current session under folder *content/datasets*. This will take some time, but You only need to run this once when starting the session.\n",
        "\n",
        "**Note that terminating the session loses the dataset (you will need to download it again by running this cell).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ljUwgdx-WQt"
      },
      "outputs": [],
      "source": [
        "# Obtaining the images from the repo\n",
        "tf.keras.utils.get_file(\n",
        "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
        "        cache_dir = '.',\n",
        "        extract=True)\n",
        "# Obtaining the captions and the dataset splits from the repo\n",
        "tf.keras.utils.get_file(\n",
        "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
        "        cache_dir='.',\n",
        "        extract=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PoTnbfg9iDB"
      },
      "source": [
        "**You can check that the dataset was downloaded by inspecting the files in the Colab navigation bar in the left**. You should find a *datasets* folder added and under which there are some text files and a *Flicker8k_Dataset* folder containing the images.\n",
        "\n",
        "*Before continuing on, please make sure that the dataset was downloaded successfully to your session without any issues due to connection, etc. if you think there is an issue, try restarting the session and re-running the data downloading cell.*\n",
        "\n",
        "The dataset is structured as follows:\n",
        "\n",
        "\n",
        "*   Each image has a certain Id and is stored in JPG format\n",
        "*   The text file *Flickr8k.token.txt* contains the captions for each image, it follows the following format: < image_name >#< caption number >    < Caption >, where each image has up to 5 captions\n",
        "*    The text files *Flickr_8k.trainImages.txt* and *Flickr_8k.testImages.txt* contain the image names for the training and testing splits respectively\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5j1cFWQ_EHW"
      },
      "source": [
        "Now, lets define some useful directories to be used throughout the Assignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT7L7y6w9J7L"
      },
      "outputs": [],
      "source": [
        "# The main directory containing the dataset\n",
        "dataset_dir = './datasets/'\n",
        "# The directory to access the images\n",
        "images_dir =  dataset_dir + 'Flicker8k_Dataset/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vTRlivjB8s5"
      },
      "source": [
        "#### 5.2.2.2 Loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Y5PEVdAcOr"
      },
      "source": [
        "**Note that the major bottleneck on the RAM is loading all the images at once, so instead we load the image names and their captions. Later we will see how we can use the image names to load the images in batches instead to avoid overflowing the RAM.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lHx1AhhdPd1"
      },
      "source": [
        "Define a method to load the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EcDpRIwIDxvp"
      },
      "outputs": [],
      "source": [
        "# Loads a file into a long string\n",
        "def load_file(file_name):\n",
        "   with open(file_name, \"r\") as fp:\n",
        "    # Read and return all text in the file\n",
        "    string = fp.read()\n",
        "    return string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6V5eJm9D1WW"
      },
      "source": [
        "Define methods to prepare the captions\n",
        "\n",
        "The format we will follow is to have a [dictionary](https://www.geeksforgeeks.org/python-dictionary/) mapping each image by its Id to a list of strings representaing the corresponding captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjJGQnBIEbfC"
      },
      "outputs": [],
      "source": [
        "# Prepares a dictionary mapping each image Id to the corresponding captions\n",
        "# This relies on the .split() method\n",
        "# Takes as input the long text read from the file by the previous method\n",
        "def prepare_captions_dict(text):\n",
        "\n",
        "  captions_dict = {}\n",
        "  # Split the file text into lines\n",
        "  lines = text.split ('\\n')\n",
        "\n",
        "  # Process each line separately to extract the (image, caption) pair\n",
        "  for line in lines:\n",
        "\n",
        "    # Split into the <imageId> and <caption>\n",
        "    line_split = line.split('\\t')\n",
        "\n",
        "    # Added this check because dataset contains some blank lines, they are ignored\n",
        "    if (len(line_split) != 2):\n",
        "      continue\n",
        "    else:\n",
        "      image, caption = line_split\n",
        "\n",
        "    # Split into <image_name> and <caption_idx>\n",
        "    image_name, caption_idx = image.split('#')\n",
        "    # Split the <image_name> into <image_id>.jpg\n",
        "    image_id = image_name.split('.')[0]\n",
        "\n",
        "    # If this is the first caption for this image, create a new list for that\n",
        "    # image and add the caption to it. Otherwise append the caption to the\n",
        "    # existing list\n",
        "    if (int(caption_idx) == 0):\n",
        "      captions_dict[image_id] = [caption]\n",
        "    else:\n",
        "      captions_dict[image_id].append(caption)\n",
        "\n",
        "  return captions_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU0E_9vUc9c4"
      },
      "source": [
        "Define methods to load the image Ids of a certain split (training/validation/testing), given the read split file (like Flickr_8k.trainImages.txt) as text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2-Ha4CUdFAQ"
      },
      "outputs": [],
      "source": [
        "# This method gets the ids of images that correspond to a certain split\n",
        "# It takes the read split file as a text\n",
        "def get_split_image_ids(text):\n",
        "\n",
        "    ids = []\n",
        "    # Split the file text into lines\n",
        "    lines = text.split ('\\n')\n",
        "    for line in lines:\n",
        "      # skip any empty lines\n",
        "      if (len(line) < 1):\n",
        "        continue\n",
        "\n",
        "      # Each line is the <image_name>\n",
        "      # Split the <image_name> into <image_id>.jpg\n",
        "      image_id = line.split ('.')[0]\n",
        "\n",
        "      # Add the <image_id> to the list\n",
        "      ids.append(image_id)\n",
        "\n",
        "    return set(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVHtXxBeDYsl"
      },
      "source": [
        "Define methods to load the image file (they will be used to visualize the images, and later on when loading the dataset for training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLP1ns7ZfpJT"
      },
      "outputs": [],
      "source": [
        "# This method loads an image as it is (mainly used for visualization purposes)\n",
        "# Note that we use RGB images so the channels should be set to 3\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlYgqpHgjFkk"
      },
      "outputs": [],
      "source": [
        "# This method loads a single image, resizes it to a certain size\n",
        "# then, preprocess_input of inception_v3 is used to preprocess the images\n",
        "# You can read more about it here: https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/preprocess_input\n",
        "def load_preprocess_image(image_path, image_size = (299,299)):\n",
        "  img = load_image(image_path)\n",
        "  img = tf.image.resize(img, image_size)\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  return img, image_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EGB73sAieOg"
      },
      "source": [
        "Loading the captions dictionary, the training, and the testing image Ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doUgNKxS_8X4"
      },
      "outputs": [],
      "source": [
        "# preparing the dictionary for the captions\n",
        "captions = prepare_captions_dict(load_file(dataset_dir + 'Flickr8k.token.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FicZVAVJcihf"
      },
      "outputs": [],
      "source": [
        "# preparing the dataset split ids\n",
        "training_image_ids = get_split_image_ids(load_file(dataset_dir + 'Flickr_8k.trainImages.txt'))\n",
        "test_image_ids = get_split_image_ids(load_file(dataset_dir + 'Flickr_8k.testImages.txt'))\n",
        "print('Number of training images: {}'.format(len(training_image_ids)))\n",
        "print('Number of testing images: {}'.format(len(test_image_ids)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbQ0rVD2iuTw"
      },
      "source": [
        "#### 5.2.3 Visualizing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU-Rho9gjy_v"
      },
      "source": [
        "You can run it different times to see other samples at random. Feel free to check different samples and see their captions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LEOQfvQi0Nn"
      },
      "outputs": [],
      "source": [
        "# Selecting a random training image Id and loading the image\n",
        "random_image_id = random.choice(tuple(training_image_ids))\n",
        "random_image_path = images_dir + random_image_id + '.jpg'\n",
        "random_image = load_image(random_image_path)\n",
        "\n",
        "# Printing the corresponding captions using the dictionary\n",
        "print('Captions:')\n",
        "for caption in captions[random_image_id]:\n",
        "  print(caption)\n",
        "\n",
        "# Displaying the image\n",
        "plt.axis('off')\n",
        "plt.imshow(random_image)\n",
        "print('\\nImage:')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTwFx4FbW253"
      },
      "source": [
        "### 5.2.3 Processing the Data for Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eWI8An_XAbK"
      },
      "source": [
        "First, let's take a step back and consider what we are doing...\n",
        "We have some images and their corresponding text captions that we want to learn how to predict automatically. We are using an encoder-decoder architecture for this, but why?\n",
        "\n",
        "We want the encoder to extract the features of the images to have a richer representation for the model. This representation can then be processed to generate the captions. However, using a simple fully-connected layer to generate the captions will be less likely to generate a linguistically correct caption, thus we rely on an RNN-based decoder that takes into consideration the previously predicted word in the caption when predicting the next word. Finally, we also incorporate an attention mechanism to improve the model performance by making it focus on the important parts in the image.\n",
        "\n",
        "To do this, we need two main operations on the data before training the model:\n",
        "\n",
        "\n",
        "*   Encode the images: this is just another way of saying extract features from them, and store those extract features to be used later on.\n",
        "*   Encode the captions: captions are originally formatted as text, however, our models cannot work directly with string representations. Thus, we need to convert the captions to a numerical representation. To do so, we extract a vocabulary from the training captions and map each element in the vocabulary to a numerical representation according to its index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXlkmEzTtJ3M"
      },
      "source": [
        "#### 5.2.3.1 Encoding the Images using a Pretrained CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMEg2dCLuxsn"
      },
      "source": [
        "We use the [InceptionV3](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/InceptionV3) CNN pre-trained on ImageNet. As we are working on normal real-life images, using the pre-trained version on imagenet makes sense.\n",
        "\n",
        "We will use the functional API [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) to initialize the feature extractor in a flexible and extensible way (**in case you wanted to combine it with the decoder later on and have an e2e encoder-decoder that can be trained in its entirety. This is not required in the assignment.**)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Fill in the following code cell by initializing an InceptionV3 model pre-trained on imagenet. **Make sure to remove the classifier part at the top of the model.**"
      ],
      "metadata": {
        "id": "k-p02jtL0oOc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgnzrKX_ugmo"
      },
      "outputs": [],
      "source": [
        "# Use a pre-trained InceptionV3 without the classifier part\n",
        "feature_extractor = '''TODO: Initialize the InceptionV3'''\n",
        "feature_extractor_model = tf.keras.Model(feature_extractor.input, feature_extractor.layers[-1].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgW4AEeivwXI"
      },
      "source": [
        "We now encode the images by loading the training images, extracting their features using the feature_extractor_model we defined and storing the extracted features as numpy files for later use.\n",
        "\n",
        "*The encoded files will be stored in the same directory of the original images with the same name for convenience along with a suffix of .npy to differentiate them from the original image files.*\n",
        "\n",
        "Not to crash the RAM, we will be using the [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to load the images, and do the following steps:\n",
        "\n",
        "\n",
        "1.   Define a certain batch_size to be loaded each time (instead of loading the whole images at once)\n",
        "2.   Instantiate a tf.data.Dataset variable by slicing the train_paths into single tensors, using [tf.data.Dataset.from_tensor_slices](https://www.geeksforgeeks.org/tensorflow-tf-data-dataset-from_tensor_slices/)\n",
        "3.   Using  [.map()](https://www.geeksforgeeks.org/tensorflow-js-tf-data-dataset-map-function/) operation to map the train_paths tensors to the corresponding images using the load_preprocess_image as the transform function\n",
        "4.   Using [.batch()](https://www.geeksforgeeks.org/tensorflow-js-tf-data-dataset-class-batch-method/) and passing in the batch_size to obtain images in batches\n",
        "\n",
        "If you feel confused still, please check this awesome [article](https://cs230.stanford.edu/blog/datapipeline/#:~:text=An%20overview%20of%20tf.data%20The%20Dataset%20API%20allows,creates%20batches%20and%20sends%20it%20to%20the%20GPU.) on how to build an efficeint dataset pipeline using tf.data. I recommend you read the *Building an image data pipeline* section carefully.\n",
        "\n",
        "After that, we use [tqdm](https://tqdm.github.io/) to loop on the batches along with a visual representation of the progress.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Fill in the missing code parts in the cell to complete the data loading process and encode the images. **Use the previous hints as guidelines.**"
      ],
      "metadata": {
        "id": "-5Icka601X6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLDXQ5rwvzAX"
      },
      "outputs": [],
      "source": [
        "# Preparing the training image paths\n",
        "training_image_paths = [images_dir + image_id + '.jpg' for image_id in training_image_ids]\n",
        "\n",
        "# remove any duplications\n",
        "train_paths = sorted(set(training_image_paths))\n",
        "\n",
        "# Load the images in batches using the steps detailed above\n",
        "# Feel free to experiment with the batch_size and see the effect on RAM and execution time\n",
        "batch_size = 16\n",
        "\n",
        "image_dataset = '''TODO: first step, instantiate the dataset using from_tensor_slices on the train_paths'''\n",
        "image_dataset = '''TODO: second step, call map() with the function that loads preprocessed images defined before'''\n",
        "image_dataset = '''TODO: last step, call batch() with the batch size defined'''\n",
        "\n",
        "# Loop on the image batches using tqdm (we loop on the batches themselves obtained using the image_dataset variable)\n",
        "for batch_images, batch_path in tqdm(image_dataset):\n",
        "  # Use the encoder to extract features from the current batches\n",
        "  batch_features = '''TODO: pass the batch of images to the feature extractor model we made'''\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  # Store the encoded images at the same path and add in a suffix of .npy (we loop now for each image in the batch)\n",
        "  for bf, p in zip(batch_features, batch_path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())\n",
        "\n",
        "print('\\n\\nCompleted Encoding the training images, encoded version has same name and path as the original with a suffix of .npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF-rRfcw28OF"
      },
      "source": [
        "#### 5.2.3.2 Encoding the Captions used for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPBTD_NwsLjq"
      },
      "source": [
        "![CNN RNN Image Captioning](https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab5/Encoding_Captions.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjO5-Hj1mxrm"
      },
      "source": [
        "This is done by the following steps:\n",
        "\n",
        "\n",
        "*   Pre-process and clean the captions text:\n",
        "\n",
        "   *   Convert all the text to lower cases to have a unified representation of the words\n",
        "   *   Remove any punctuation and words which are one character or less in length (such as 'a' and ' ')\n",
        "   *   Remove any alphanumeric symbols (any words with numbers in them)\n",
        "   *   Add in a starting and ending token to represent the start and end of the caption sentence.\n",
        "\n",
        "*   Obtain only the training captions to get the vocabulary from (we shouldn't leak any information from the testing set so the training set is used for this)\n",
        "\n",
        "*   Tokenize the training captions to prepare a vocabulary of the used words encoded numerically"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svluy9Rd3eKI"
      },
      "source": [
        "##### Preprocessing and cleaning the captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0UymrWcoEfW"
      },
      "source": [
        "Using [re](https://docs.python.org/3/library/re.html) to perform regex operations for cleaning the captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BsnYaa-Myejf"
      },
      "outputs": [],
      "source": [
        "# This function processes the captions inplace\n",
        "# Takes as input the dictionary mapping the image Ids to the corresponding captions\n",
        "def captions_clean(caption_dict):\n",
        "\n",
        "  for _, captions in caption_dict.items():\n",
        "\n",
        "    # Loop through each caption for each image\n",
        "    for i, caption in enumerate(captions):\n",
        "\n",
        "      # Convert the caption to lowercase, and then remove all special characters from it (those having only one character)\n",
        "      caption_nopunc = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
        "\n",
        "      # Split the caption into separate words, and collect all words which are more than\n",
        "      # one character and which contain only alphabets (ie. discard words with mixed alpha-numerics)\n",
        "      clean_words = [word for word in caption_nopunc.split() if ((len(word) > 1) and (word.isalpha()))]\n",
        "\n",
        "      # Join those words into a string\n",
        "      caption_final = ' '.join(clean_words)\n",
        "\n",
        "      # Replace the old caption in the captions list with this new cleaned caption\n",
        "      captions[i] = caption_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvnLH9kW4GBB"
      },
      "source": [
        "Adding in start and end tokens for the captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzvO75gDozsP"
      },
      "outputs": [],
      "source": [
        "# The tokens constant definition, used throughout the notebook\n",
        "START_TOKEN = 'starttok'\n",
        "END_TOKEN = 'endtok'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URm8ALlq4N8k"
      },
      "outputs": [],
      "source": [
        "def add_tokens(captions):\n",
        "  for i, caption in enumerate (captions):\n",
        "    captions[i] = START_TOKEN + ' ' + caption + ' ' + END_TOKEN\n",
        "  return captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUL-rtQs4v17"
      },
      "source": [
        "##### Divide the captions according to the dataset splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg3sn5od4jpa"
      },
      "outputs": [],
      "source": [
        "# Used to get the captions corresponding to a certain split of the dataset (training/testing)\n",
        "# Will be used when calling the functions to obtain the training split\n",
        "def subset_caption_dict (caption_dict, image_Ids):\n",
        "  dict = { image_Id:add_tokens(captions) for image_Id,captions in caption_dict.items() if image_Id in image_Ids}\n",
        "  return dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bfiBAOxp5Oc"
      },
      "source": [
        "##### Tokenizing the captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp-kuBGT5GI8"
      },
      "source": [
        "Fitting a [Keras tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to the captions to be used to get the numeric representations of the captions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** fill in the missing code to perform the tokenization on the captions."
      ],
      "metadata": {
        "id": "Hka18qFf3BOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAR05the5PCV"
      },
      "outputs": [],
      "source": [
        "def create_tokenizer(caption_dict):\n",
        "  # Get a flat list of all the captions from the dictionary\n",
        "  captions = [caption for key, captions in caption_dict.items() for caption in captions]\n",
        "\n",
        "  # Getting the number of words in the caption with the most words\n",
        "  max_caption_words = max(len(caption.split()) for caption in captions)\n",
        "\n",
        "  # Initialise a Keras Tokenizer\n",
        "  tokenizer = '''TODO: Initialize a keras tokenizer'''\n",
        "\n",
        "  # Fit it on the captions so that it prepares a vocabulary of all words\n",
        "  '''TODO: fit the tokenizer on the captions texts'''\n",
        "\n",
        "  # Get the size of the vocabulary\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  return tokenizer, vocab_size, max_caption_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQSiMf2358B6"
      },
      "source": [
        "Pad a list of text indeces to a fixed length. Why do we need this?\n",
        "\n",
        "We want a fixed-length vector numerical representation of the captions, however, captions have different lengths. A solution would be to pad all captions by zeros to have a fixed length equal to the length of the caption with the maximum number of words (obtained from the previous function). Padding by zero here indicates that there are no words in this index of the vector representation.\n",
        "\n",
        "Done using [pad_sequences](https://tensorflow.google.cn/api_docs/python/tf/keras/utils/pad_sequences) of keras."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Fill in the following cell to perform the padding"
      ],
      "metadata": {
        "id": "QDrgy5dL3khI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-GRgzgw6Gqq"
      },
      "outputs": [],
      "source": [
        "def pad_text(string, max_length):\n",
        "  '''TODO: call pad_sequences with the appropriate parameters.\n",
        "            Note that the seqeunces parameter expects a list of sequences (list of strings), that's why I passed [string] and not string\n",
        "            Similarly, it returns a list of padded strings, that's why I return pad_sequences()[0]\n",
        "            You need to pass the correct max length to the maxlen parameter\n",
        "            We want the padding to be at the end of the seqeunce, i.e., after each sequence (hint: look into the padding parameter possible values in the provided link)\n",
        "  '''\n",
        "  string = pad_sequences([string], maxlen= '''TODO: define the maxlen parameter''', padding='''TODO: define the padding parameter''')[0]\n",
        "  return string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhut-CSP6VGH"
      },
      "source": [
        "##### Performing the function calls on the captions dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rmpLwhJ96YSu"
      },
      "outputs": [],
      "source": [
        "# This function processes the captions inplace\n",
        "captions_clean(captions)\n",
        "training_dict = subset_caption_dict(captions, training_image_ids)\n",
        "\n",
        "# Prepare tokenizer\n",
        "tokenizer, vocab_size, max_caption_words = create_tokenizer(training_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_ybrNN87OFt"
      },
      "source": [
        "### 5.2.4 Obtaining the Input/Output Data for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ45pat_7y_D"
      },
      "source": [
        "Remember that the data is lazily loaded to prevent memory issues, to do so we first prepare training (x,y) pairs where x is the image file path and y are the corresponding cleaned and tokenized captions.\n",
        "\n",
        "Then, we wil use tf.data to load the images in batches similar to what was done when using InceptionV3 to perform feature extraction earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLhHq7AM6tyh"
      },
      "outputs": [],
      "source": [
        "#  Getting the training X and Y pairs\n",
        "def data_prep(caption_dict, tokenizer, max_length, vocab_size):\n",
        "  X, y = list(), list()\n",
        "\n",
        "  # For each image and list of captions\n",
        "  for image_id, captions in caption_dict.items():\n",
        "    image_id = images_dir + image_id + '.jpg'\n",
        "\n",
        "    # For each caption in the list of captions\n",
        "    for caption in captions:\n",
        "\n",
        "      # Convert the caption words into a list of word indices\n",
        "      word_idxs = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "      # Pad the input text to the same fixed length\n",
        "      pad_idxs = pad_text(word_idxs, max_length)\n",
        "\n",
        "      X.append(image_id)\n",
        "      y.append(pad_idxs)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "train_X, train_y = data_prep(training_dict, tokenizer, max_caption_words, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGIQFMLC9LC8"
      },
      "source": [
        "Load the pre-processed encoded images saved in an earlier stage (the ones encoded using the pre-trained CNN)\n",
        "\n",
        "*Done using tf.data with the same approach as in the feature extractor part when encoding the images.*\n",
        "\n",
        "**Please refer back to that section and follow the same steps indicated there.**\n",
        "\n",
        "The main difference here is that the tensors we are working on are the pairs (X,Y). This will change the way we specify the arguments of the from_tensor_slices(). Also this changes how we use the map() operation, the mapping function now returns a pair of variables (the image and the corresponding caption), thus we use the [lambda expression](https://www.geeksforgeeks.org/python-lambda/) along with the [tf.numpy_function](https://www.tensorflow.org/api_docs/python/tf/numpy_function) to define the mapping function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Fill in the cell to perform the lazy data loading similar to what you did before."
      ],
      "metadata": {
        "id": "Ip54hJHV4aZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl0urDBX9Kj4"
      },
      "outputs": [],
      "source": [
        "# Feel free to experiment with different batch sizes\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# Mapping function used to Load the encoded image files (the extracted features)\n",
        "def map_func(img_name, cap):\n",
        "   img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "   return img_tensor, cap\n",
        "\n",
        "# Obtaining Input/Output pair slices from the X and Y lists\n",
        "dataset = '''TODO: Initialize the dataset using from_tensor_slices on train_X and train_y obtained in the cell above'''\n",
        "\n",
        "# Use map to load the encoded image files\n",
        "# TODO: Fill in the map() call to call the mapping function\n",
        "dataset = dataset.map(lambda curr_img_name, curr_cap: tf.numpy_function('''TODO: add the mapping function defined''',\n",
        "                                                                    '''TODO: add the arguments input to the mapping function''',\n",
        "                                                                    [tf.float32, tf.int32]))\n",
        "\n",
        "# Shuffle and Batch the dataset\n",
        "dataset = '''TODO: call shuffle using the defined buffer size'''\n",
        "dataset = '''TODO: call batch using the defined batch size'''\n",
        "# Prefetch from the dataset\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U50E-FJZq9OI"
      },
      "source": [
        "### 5.2.5 The Encoder-Decoder Model with Attention and Pre-trained Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYR9KIpgrS9c"
      },
      "source": [
        "The model is prepared through the use of [python classes](https://www.geeksforgeeks.org/python-classes-and-objects/) for ease of re-usability and extensibility.\n",
        "\n",
        "The model consists of the following main modules\n",
        "\n",
        "\n",
        "*   Encoder to encode the images.\n",
        "*   Attention to focus on the important image features.\n",
        "*   Decoder to predict the caption.\n",
        "\n",
        "The decoder outputs a **probability distribution** over the vocabulary for each timestep to predict the word of this timestep. We will use from_logits = True in the loss when training the model, hence the output of the decoder won't have a softmax activation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQKZbSnjY7V9"
      },
      "source": [
        "![Model](https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab5/Model_full.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2dRKRxDrJEi"
      },
      "source": [
        "#### 5.2.5.1 Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2UAKz3drgr-"
      },
      "source": [
        "As we already encoded the images using a pre-trained CNN, this module is very simple. We just add a Fully Connected [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer after the encoded images we saved."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Fill in the code to define the encoder"
      ],
      "metadata": {
        "id": "Hj1avIKe5d3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URUUQIc0o9mG"
      },
      "outputs": [],
      "source": [
        "class Pretrained_Encoder(tf.keras.Model):\n",
        "    # Since we have already extracted the features and saved them\n",
        "    # We just pass the features through a fully connected dense layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(Pretrained_Encoder, self).__init__()\n",
        "        # shape after fc ==> (batch_size, 64, embedding_dim)\n",
        "        self.fc = '''TODO: Define a dense layer of embedding_dim units and with no activation function, it will be added later in the call'''\n",
        "\n",
        "    def call(self, x):\n",
        "        x = '''TODO: call the dense layer on x'''\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6e_8tGpt0kB"
      },
      "source": [
        "#### 5.2.5.2 Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1bffmSot6Hj"
      },
      "source": [
        "For the attention layer, we implement the [Bahdanau Additive Attention Mechanism](https://machinelearningmastery.com/the-bahdanau-attention-mechanism/), this follows similar operations to what you learned in class.\n",
        "\n",
        "We use the following tensorflow methods for the implementation:\n",
        "\n",
        "\n",
        "*   [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers to represent the weights used in the calculations\n",
        "*   [tf.expand_dims](https://www.tensorflow.org/api_docs/python/tf/expand_dims): to give the previous state an extra dimension representing the time axis\n",
        "*   [tf.math.tanh](https://www.tensorflow.org/api_docs/python/tf/math/tanh) and [tf.nn.softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) to perform the tanh and softmax operations\n",
        "*   [tf.math.reduce_sum](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum) to get the weighted sum at the end of the calculation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFLn8glebtgx"
      },
      "source": [
        "![Model](https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab5/Additive_Attention.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Fill in the code to define the attention mechanism following the figure above"
      ],
      "metadata": {
        "id": "7q4VDwTN52lE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg4JpHvbuR11"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    # These dense layers will be used as the weights used in the additive attention calculations\n",
        "    # Note that the dense layers shouldn't have an activation as the activation functions are called later according to the figure\n",
        "    self.W1 = '''TODO: Define a dense layer with \"units\" as number of units to represent W1'''\n",
        "    self.W2 = '''TODO: Define a dense layer with \"units\" as number of units to represent W2'''\n",
        "    self.V =  '''TODO: Define a dense layer with 1 unit to represent V'''\n",
        "\n",
        "  def call(self, features, prev_state):\n",
        "    # features i.e: encoder output, shape ==> (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # prev_state shape ==> (batch_size, prev_state_size)\n",
        "    # prev_state_with_time_axis shape ==> (batch_size, 1, prev_state_size)\n",
        "    prev_state_with_time_axis = tf.expand_dims(prev_state, 1)\n",
        "\n",
        "    # First, calculating the score using the encoded features and the previous state\n",
        "\n",
        "    # attention_hidden_layer shape ==> (batch_size, 64, units)\n",
        "    # This layer represents tanh(W1.h + W2.s_(t-1))\n",
        "    attention_hidden_layer = '''TODO: call tanh() on the W1.h + W2.s_(t-1)\n",
        "                                HINT: h is the features and s_(t-1) is the previous state after adding the time axis\n",
        "                                HINT: think of the dense layer as an application of the weights it represents'''\n",
        "\n",
        "    # score ie: e in the figure, shape ==> (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    # This layer represents VT(tanh(W1.h + W2.s_(t-1)))\n",
        "    score = '''TODO: perform V.tanh(W1.h + W2.s_(t-1))\n",
        "               HINT: the previous step calculated tanh(W1.h + W2.s_(t-1)\n",
        "               HINT: VT == V as it is a single unit'''\n",
        "\n",
        "    # Second, calculating the attention weights from the obtained scores\n",
        "\n",
        "    # attention_weights ie: alpha in the figure, shape == (batch_size, 64, 1)\n",
        "    # Applying a softmax to the scores gives the corresponding weight value\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # Third, multiplying the features with the attention weights and obtaining the weighted sum\n",
        "\n",
        "    # context_vector ie: c_t in the figure, shape after sum == (batch_size, prev_state_size)\n",
        "    context_vector = '''TODO: Mutiply the attention weights with the features'''\n",
        "    context_vector = '''TODO: call reduce_sum to get the weighted sum of the context_vector\n",
        "                        HINT: we want reduce_sum to reduce each sample on its own not the whole batch\n",
        "                        HINT: Consider the dimensions tracked in the code and the axis parameter of reduce_sum'''\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlMu-buGsm7Y"
      },
      "source": [
        "#### 5.2.5.3 Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtX79DS5spot"
      },
      "source": [
        "An RNN-based decoder using LSTMs using:\n",
        "\n",
        "\n",
        "*   [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): an embedding layer to embed the words using the vocabulary processed by the tokenizer in the previous steps, converting them into fixed-size dense vectors based on the vocabulary indeces.\n",
        "*   [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): for the LSTM model of the decoder\n",
        "*   [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers: to output the predicted word per step from the vocabulary. **Note that the predicted caption is a sequence of predicted words from the vocabulary.**\n",
        "\n",
        "*   The attention model we defined above\n",
        "*   [tf.concat](https://www.tensorflow.org/api_docs/python/tf/concat): to concatenate the context vector with the output of the previous step\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-manqY4Dh_C4"
      },
      "source": [
        "![Model](https://github.com/sherifmost/DeepLearning/blob/master/Labs/lab5/Decoder_LSTM.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Fill in the code to define the decoder."
      ],
      "metadata": {
        "id": "5V-Zxsd08xY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHVEnkcJspDr"
      },
      "outputs": [],
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    # Number of units used for the LSTM and the first dense layer\n",
        "    self.units = units\n",
        "\n",
        "    # The embedding layer for the words\n",
        "    self.embedding = '''TODO: Initialize an embedding layer with the appropriate vocabulary size and embedding dimension'''\n",
        "\n",
        "    # The lstm layer\n",
        "    self.lstm = '''TODO: Initialize an LSTM with \"units\" as the number of units\n",
        "                   HINTS: you need both return seqeunces and return state to be true.\n",
        "                   Return sequences allows us to obtain a many output (many words for the captions) reliably.\n",
        "                   Return state allows us to obtain the hidden state used in the attention mechanism.\n",
        "                   Use recurrent_initializer='glorot_uniform' for better convergence'''\n",
        "\n",
        "    # A hidden dense layer\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "\n",
        "    # The output dense layer of the Decoder\n",
        "    self.fc2 = '''TODO: Define the output dense layer with the correct number of outputs\n",
        "                  HINT: What is the number of possible words to output?'''\n",
        "\n",
        "    # The attention layer to be used\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  # y here represents the output of the previous step at first\n",
        "  # After the processing, it will include the output of this step\n",
        "  # Think of it as a recursive process\n",
        "  def call(self, y, features, prev_state):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = '''TODO: Call the attention layer defined, giving it the appropriate arguments'''\n",
        "\n",
        "    # y shape after passing through embedding ==> (batch_size, 1, embedding_dim)\n",
        "    # passing the output of the previous step to the embedding layer\n",
        "    y = '''TODO: Call the embedding layer defined on y'''\n",
        "\n",
        "    # y shape after concatenation ==> (batch_size, 1, embedding_dim + state_size)\n",
        "    # Concatenate y with the context_vector obtained through the attention mechanism\n",
        "    y = tf.concat([tf.expand_dims(context_vector, 1), y], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the LSTM\n",
        "    # we will use the output in the next layers, the state will be returned to be used in the attention for the next step\n",
        "    # The third returned variable is the last cell state, we will only use the last hidden state for the state variable\n",
        "    output, state, _ = '''TODO: Call the LSTM layer defined on y'''\n",
        "\n",
        "    # shape ==> (batch_size, max_length, hidden_size)\n",
        "    # pass the LSTM output to the hidden layer\n",
        "    y = self.fc1(output)\n",
        "\n",
        "    # y shape ==> (batch_size * max_length, hidden_size)\n",
        "    # reshape y\n",
        "    y = tf.reshape(y, (-1, y.shape[2]))\n",
        "\n",
        "    # output shape ==> (batch_size * max_length, vocab_size)\n",
        "    y = '''TODO: call the defined output dense layer on y'''\n",
        "\n",
        "    return y, state\n",
        "\n",
        "  # This is used to reset the state of the LSTM\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNv-05U4ysaH"
      },
      "source": [
        "### 5.2.6 Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npi1BOTxkjmk"
      },
      "source": [
        "To train the model, we will rely on tensorflow operations, with the following steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPDv6TiB01oR"
      },
      "source": [
        "#### 5.2.6.1 Initializing the Encoder and the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLxUnRHDyqAR"
      },
      "outputs": [],
      "source": [
        "# Dense layer units of the encoder\n",
        "# and the embedding dimension of the embedding layer in the decoder\n",
        "embedding_dim = 256\n",
        "# LSTM units\n",
        "units = 512\n",
        "# The number of training steps per epoch\n",
        "num_steps = len(train_X) // BATCH_SIZE\n",
        "# The vocab_size (obtained a few steps back)\n",
        "vocab_size = vocab_size\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TncXVDmz03k"
      },
      "outputs": [],
      "source": [
        "# Initializing the Encoder and the Decoder\n",
        "encoder = Pretrained_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXQ0EIcK05Sc"
      },
      "source": [
        "#### 5.2.6.2 Defining the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4xdx53FlAWO"
      },
      "source": [
        "We will be using adam optimizer and [sparseCategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy). As we set from_logits = True in the loss, there is no need to add softmax activation in the output dense layer of the decoder (as mentioned before)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWyma_c-1sv-"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer used\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAJRKKegme81"
      },
      "source": [
        "A question here, do we want to penalize the model if it doesn't predict a padding in the captions? No, as captions have varying lengths so we only want to penalize the model in case the ground truth caption had a word in this step. To do so we will need to implement a basic custom loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVHqusl1lNnv"
      },
      "source": [
        "Let's use tf operations to implement the loss function based on sparseCategoricalCrossentropy. To do so, follow the steps:\n",
        "\n",
        "\n",
        "*   Define a mask by checking that the ground truth (real) is not equal to zero using [tf.math.logical_not](https://www.tensorflow.org/api_docs/python/tf/math/logical_not) and [tf.math.equal](https://www.tensorflow.org/api_docs/python/tf/math/equal) => we don't want to penalize wrongly predicted paddings\n",
        "*   Calculate the custom loss as the sparseCategoricalCrossentropy between the real and the predicted using the loss_object we defined\n",
        "*   Apply the mask by multiplying the loss with it. By this we only calculate the sparseCategoricalCrossentropy for the parts that aren't paddings.\n",
        "*   Return the average of the loss by using [tf.reduce_mean](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Fill in the code to define the custom loss function"
      ],
      "metadata": {
        "id": "1FaOUiu1-aWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2Cl3XKxz4lw"
      },
      "outputs": [],
      "source": [
        "# The loss function used\n",
        "def loss_function(real, pred):\n",
        "  mask = '''TODO: Add a check that real is not equal to zero\n",
        "            HINTS: Use equal() to check if real is equal to zero and wrap it with logical_not()'''\n",
        "  loss_ = '''TODO: Call loss_object passing the approapriate arguments'''\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StyZg-gymylj"
      },
      "source": [
        "Now, lets implement the full training step. We will mainly rely on the loss function we just implemented and on [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape). The training step calculates the loss and performs the weight updates for a single batch of trainnig samples.\n",
        "\n",
        "We will also use [teacher forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/) when training the decoder for a more stable training by using the previous timestep from the ground truth not from the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Fill in the code to complete the training process"
      ],
      "metadata": {
        "id": "2DbFf4EI_HCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDYrTt5t0_yE"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target, encoder, decoder):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image, so we need to reset the state\n",
        "  state = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  # The first token to be given to the decoder is the START_TOKEN\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index[START_TOKEN]] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      # Call the encoder model on the image to get the encoded features\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, state= '''TODO: Call the decoder passing in the appropriate arguments\n",
        "                                 HINT: Revise the decoder class we defined\n",
        "                                 HINT: for the first step we pass the START_TOKEN to the decoder,\n",
        "                                 this was defined using the dec_input variable in this cell'''\n",
        "          # Append to the loss the current loss calculated between the predictions and the ground truth target up to this point\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  # The trainable variables to be passed to the gradient tape are both the encoder and decoder trainable variables\n",
        "  trainable_variables = '''TODO: combine both the encoder and the decoder trainable variables\n",
        "                           HINTS: use .trainable_variables, you can combine them using by concatenations or using + as a concatenation operation'''\n",
        "\n",
        "  # pass the trainable variables and the loss to the gradient tape\n",
        "  gradients = '''TODO: call tape.gradient passing in the loss and the trainable variables to update the weights'''\n",
        "\n",
        "  # Use the optimizer to apply gradient decent and update the weights of the trainable variables\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  # Get the average loss\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  return loss, total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRUjOpmfpiOK"
      },
      "source": [
        "Perform the training and monitor the loss. To perform the training, we call the training step for each batch of each epoch. The training batches are lazily loaded using the dataset variable we defined a few steps back when obtaining the input/output data for training (the batch size was defined in that step too)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Run this cell and keep the output."
      ],
      "metadata": {
        "id": "eIuAJJ_OCmo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xCCV7GiN2NO3"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "EPOCHS = 20\n",
        "loss_plot = []\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start_time = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target, encoder, decoder)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for this epoch {time.time()-start_time:.2f} sec\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haKEXyBHJZTJ"
      },
      "source": [
        "Plot the loss to make sure that nothing wrong happened while training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Run this cell and keep the output."
      ],
      "metadata": {
        "id": "EbevMPdJCqqQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rDKbNy3BOTl"
      },
      "outputs": [],
      "source": [
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.plot(np.arange(1,21),loss_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzrIE2blCBxe"
      },
      "source": [
        "### 5.2.7 Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6zVapAEFVzc"
      },
      "source": [
        "We will rely on qualitative evaluation by inspecting the output captions corresponding to random test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_USZBVX1CLQ6"
      },
      "outputs": [],
      "source": [
        "# This method takes an image and obtains the predicted captions\n",
        "# This is done by using the encoder and decoder models and obtaining the predicted words by sampling from\n",
        "# the predicted random distribution output by the decoder at each time step.\n",
        "# The predicted word of timestep i is passed to timestep i+1 along with the extracted image features to predict the next word.\n",
        "# The predictions stop when the model outputs END_TOKEN or when it outputs a caption with the maximum length.\n",
        "def evaluate(image, max_length, encoder, decoder):\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_preprocess_image(image)[0], 0)\n",
        "    img_tensor_val = feature_extractor_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                 -1,\n",
        "                                                 img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index[START_TOKEN]], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden = decoder(dec_input,\n",
        "                                      features,\n",
        "                                      hidden)\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == END_TOKEN:\n",
        "            return result\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR1hbyg89f8R"
      },
      "outputs": [],
      "source": [
        "def check_test(test_image_names, image_dict, image_dir, max_caption_words, encoder, decoder):\n",
        "  # captions on the validation set\n",
        "  rid = np.random.randint(0, len(test_image_names))\n",
        "  image_name = test_image_names[rid]\n",
        "\n",
        "  real_caption = image_dict[image_name]\n",
        "\n",
        "  image_path = image_dir + image_name + '.jpg'\n",
        "  result = evaluate(image_path, max_caption_words, encoder, decoder)\n",
        "\n",
        "  # Displaying the image\n",
        "  original_image = load_image(image_path)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(original_image)\n",
        "  print('Real Captions:')\n",
        "  for caption in real_caption:\n",
        "    print(' ' + caption)\n",
        "  print('\\nPrediction Caption:\\n', ' '.join(result))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frGFgUElIO3D"
      },
      "source": [
        "Test the model on a random image from the testing set, run the following cell different times to see predictions for different random test images!\n",
        "\n",
        "Neglect the start and end tokens in the output predicted captions if they are outputted.\n",
        "\n",
        "**Feel free to experiment more with the model parameters and training time to generate even better captions!**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Run this cell and keep the output"
      ],
      "metadata": {
        "id": "htoLi0Y0Aua7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl5agnKSCPME"
      },
      "outputs": [],
      "source": [
        "test_image_name_file = dataset_dir + \"Flickr_8k.testImages.txt\"\n",
        "test_image_ids = get_split_image_ids(load_file(test_image_name_file))\n",
        "result = check_test(list(test_image_ids), captions, images_dir, max_caption_words, encoder, decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1NPM797C_5V"
      },
      "source": [
        "## 5.3 Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPhiXDFhrim8"
      },
      "source": [
        "That's it! Congratulations on training an image captioning model.\n",
        "\n",
        "Make sure you deliver all the requirements for the submission and to keep the outputs in the notebook!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}